{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "## Description\n",
    "Hyperparameter tuning is a crucial step in the machine learning pipeline. It involves selecting the best set of hyperparameters for a machine learning model to achieve optimal performance. Hyperparameters are parameters that are not learned from the data but are set before the training process begins. Examples of hyperparameters include learning rate, batch size, and the number of hidden layers in a neural network.\n",
    "\n",
    "## Importance\n",
    "Choosing the right hyperparameters can significantly impact the performance of a machine learning model. Poorly chosen hyperparameters can lead to underfitting or overfitting, resulting in suboptimal model performance. Hyperparameter tuning helps in finding the best combination of hyperparameters that maximize the model's performance on a given dataset.\n",
    "\n",
    "## Techniques\n",
    "There are several techniques for hyperparameter tuning, including:\n",
    "\n",
    "### Grid Search\n",
    "Grid search is a brute-force approach to hyperparameter tuning. It involves defining a grid of hyperparameter values and evaluating the model for each combination of hyperparameters. This method can be computationally expensive but guarantees finding the best combination within the defined grid.\n",
    "\n",
    "### Random Search\n",
    "Random search is a more efficient alternative to grid search. Instead of evaluating all possible combinations, it randomly samples a subset of hyperparameter combinations. This method can be faster and often finds good hyperparameter combinations with fewer evaluations.\n",
    "\n",
    "### Bayesian Optimization\n",
    "Bayesian optimization is a probabilistic model-based approach to hyperparameter tuning. It builds a surrogate model to approximate the objective function and uses this model to select the most promising hyperparameter combinations to evaluate. This method can be more efficient than grid search and random search, especially for high-dimensional hyperparameter spaces.\n",
    "\n",
    "### Hyperband\n",
    "Hyperband is a bandit-based approach to hyperparameter tuning. It dynamically allocates resources to different hyperparameter configurations based on their performance. This method can be more efficient than grid search and random search, especially when the evaluation of hyperparameter configurations is expensive.\n",
    "\n",
    "## Practice\n",
    "For practical examples and exercises on hyperparameter tuning techniques, refer to the [Hyperparameter Tuning Notebook](01_hyperparameter_tuning.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best parameters found by Grid Search:\", grid_search.best_params_)\n",
    "\n",
    "# Define hyperparameters for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "print(\"Best parameters found by Random Search:\", random_search.best_params_)\n",
    "\n",
    "# Define hyperparameters for Bayesian Optimization\n",
    "search_spaces = {\n",
    "    'n_estimators': Integer(10, 200),\n",
    "    'max_depth': Integer(10, 50),\n",
    "    'min_samples_split': Integer(2, 15),\n",
    "    'min_samples_leaf': Integer(1, 6)\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search = BayesSearchCV(estimator=model, search_spaces=search_spaces, n_iter=32, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "bayes_search.fit(X, y)\n",
    "print(\"Best parameters found by Bayesian Optimization:\", bayes_search.best_params_)\n"
   ]
  }
 ]
}
