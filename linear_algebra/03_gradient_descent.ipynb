{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "## Explanation\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts the model parameters to find the optimal values that minimize the cost function.\n",
    "\n",
    "### Importance in Machine Learning\n",
    "Gradient descent is crucial in training machine learning models, especially in deep learning. It helps in finding the optimal parameters that minimize the error between the predicted and actual values.\n",
    "\n",
    "### Gradient Descent Algorithms\n",
    "1. **Batch Gradient Descent**: Uses the entire dataset to compute the gradient and update the parameters.\n",
    "2. **Stochastic Gradient Descent (SGD)**: Uses a single data point to compute the gradient and update the parameters.\n",
    "3. **Mini-batch Gradient Descent**: Uses a small batch of data points to compute the gradient and update the parameters.\n",
    "\n",
    "### Examples\n",
    "- **Batch Gradient Descent**: If the cost function is J(θ), the parameters are updated as θ = θ - α * ∇J(θ), where α is the learning rate.\n",
    "- **Stochastic Gradient Descent**: If the cost function is J(θ), the parameters are updated as θ = θ - α * ∇J(θ; x^(i)), where x^(i) is a single data point.\n",
    "- **Mini-batch Gradient Descent**: If the cost function is J(θ), the parameters are updated as θ = θ - α * ∇J(θ; X^(i)), where X^(i) is a small batch of data points.\n",
    "\n",
    "## Practice\n",
    "For practice, refer to the [Gradient Descent Notebook](03_gradient_descent.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cost function\n",
    "def cost_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    J = np.sum((X.dot(theta) - y) ** 2) / (2 * m)\n",
    "    return J\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        theta = theta - (alpha / m) * X.T.dot(X.dot(theta) - y)\n",
    "        J_history.append(cost_function(X, y, theta))\n",
    "    return theta, J_history\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "theta = np.array([0, 0])\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "# Perform gradient descent\n",
    "theta, J_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
    "\n",
    "# Plot the cost function history\n",
    "plt.plot(J_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function History')\n",
    "plt.show()\n",
    "\n",
    "print('Theta:', theta)\n",
    "print('Final cost:', J_history[-1])\n"
   ]
  }
 ]
}
